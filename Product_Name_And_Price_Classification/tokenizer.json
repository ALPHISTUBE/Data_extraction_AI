"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 10000, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": null, \"document_count\": 1, \"word_counts\": \"{\\\"sample\\\": 1, \\\"text\\\": 1}\", \"word_docs\": \"{\\\"text\\\": 1, \\\"sample\\\": 1}\", \"index_docs\": \"{\\\"2\\\": 1, \\\"1\\\": 1}\", \"index_word\": \"{\\\"1\\\": \\\"sample\\\", \\\"2\\\": \\\"text\\\"}\", \"word_index\": \"{\\\"sample\\\": 1, \\\"text\\\": 2}\"}}"